{"cells": [{"cell_type": "code", "metadata": {}, "source": ["import pandas as pd\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.stem import WordNetLemmatizer\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n\n", "nltk.download('punkt')\n", "nltk.download('stopwords')\n", "nltk.download('wordnet')\n\n", "data = {\n", "    'text': [\n", "        'Artificial Intelligence is changing the world.',\n", "        'Machine learning models need a lot of data.',\n", "        'NLP helps computers understand human language.'\n", "    ]\n", "}\n\n", "df = pd.DataFrame(data)\n", "df['tokens'] = df['text'].apply(nltk.word_tokenize)\n", "stop_words = set(stopwords.words('english'))\n", "df['filtered_tokens'] = df['tokens'].apply(lambda x: [w for w in x if w.lower() not in stop_words])\n", "lemmatizer = WordNetLemmatizer()\n", "df['lemmatized'] = df['filtered_tokens'].apply(lambda x: [lemmatizer.lemmatize(w.lower()) for w in x])\n", "tfidf = TfidfVectorizer()\n", "tfidf_matrix = tfidf.fit_transform(df['text'])\n", "print(tfidf.get_feature_names_out())\n", "print(tfidf_matrix.toarray())\n", "df"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 0}